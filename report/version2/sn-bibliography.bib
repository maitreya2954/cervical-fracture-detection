%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Siddharth Rayabharam at 2022-12-08 17:31:48 -0500 


%% Saved with string encoding Unicode (UTF-8) 



@article{Mawatari:2020aa,
	abstract = {PURPOSE: The purpose of our study is to develop deep convolutional neural network (DCNN) for detecting hip fractures using CT and MRI as a gold standard, and to evaluate the diagnostic performance of 7 readers with and without DCNN. METHODS: The study population consisted of 327 patients who underwent pelvic CT or MRI and were diagnosed with proximal femoral fractures. All radiographs were manually checked and annotated by radiologists referring to CT and MRI for selecting ROI. At first, a DCNN with the GoogLeNet model was trained by 302 cases. The remaining 25 cases and 25 control subjects were used for the observer performance study and for the testing of DCNN. Seven readers took part in this study. A continuous rating scale was used to record each observer's confidence level. Subsequently, each observer interpreted with the DCNN outputs and rated them again. The area under the curve (AUC) was used to compare the fracture detection. RESULTS: The average AUC of the 7 readers was 0.832. The AUC of DCNN alone was 0.905. The average AUC of the 7 readers with DCNN outputs was 0.876. The AUC of readers with DCNN output were higher than those without(p < 0.05). The AUC of the 2 experienced readers with DCNN output exceeded the AUC of DCNN alone. CONCLUSION: For detecting the hip fractures on radiographs, DCNN developed using CT and MRI as a gold standard by radiologists improved the diagnostic performance including the experienced readers.},
	address = {Department of Radiological Sciences, Graduate school of Health Sciences, Teikyo University, 6-22 Misakimachi, Omuta, Fukuoka, 836-8505, Japan.; Department of Radiology, University of Occupational and Environmental Health, 1-1, Iseigaoka, Yahatanishiku, Kitakyushu, Fukuoka, 807-8555, Japan. Electronic address: yhlinda@med.uoeh-u.ac.jp.; Department of Radiological Sciences, Graduate school of Health Sciences, Teikyo University, 6-22 Misakimachi, Omuta, Fukuoka, 836-8505, Japan.; Department of Radiology, University of Occupational and Environmental Health, 1-1, Iseigaoka, Yahatanishiku, Kitakyushu, Fukuoka, 807-8555, Japan.; Department of Radiology, University of Occupational and Environmental Health, 1-1, Iseigaoka, Yahatanishiku, Kitakyushu, Fukuoka, 807-8555, Japan.; Department of Radiology, University of Occupational and Environmental Health, 1-1, Iseigaoka, Yahatanishiku, Kitakyushu, Fukuoka, 807-8555, Japan.; Department of Radiology, University of Occupational and Environmental Health, 1-1, Iseigaoka, Yahatanishiku, Kitakyushu, Fukuoka, 807-8555, Japan.; Department of Medicine and Biosystemic Sciences, Graduate School of Medical Sciences, Kyushu University, Japan.; Department of Radiology, University of Occupational and Environmental Health, 1-1, Iseigaoka, Yahatanishiku, Kitakyushu, Fukuoka, 807-8555, Japan.; Department of Radiology, University of Occupational and Environmental Health, 1-1, Iseigaoka, Yahatanishiku, Kitakyushu, Fukuoka, 807-8555, Japan.; Department of Radiology, University of Occupational and Environmental Health, 1-1, Iseigaoka, Yahatanishiku, Kitakyushu, Fukuoka, 807-8555, Japan.; Department of Radiology, University of Occupational and Environmental Health, 1-1, Iseigaoka, Yahatanishiku, Kitakyushu, Fukuoka, 807-8555, Japan.; Department of Medicine and Biosystemic Sciences, Graduate School of Medical Sciences, Kyushu University, Japan.; Department of Radiology, University of Occupational and Environmental Health, 1-1, Iseigaoka, Yahatanishiku, Kitakyushu, Fukuoka, 807-8555, Japan.; Department of Radiology, University of Occupational and Environmental Health, 1-1, Iseigaoka, Yahatanishiku, Kitakyushu, Fukuoka, 807-8555, Japan.},
	author = {Mawatari, Tsubasa and Hayashida, Yoshiko and Katsuragawa, Shigehiko and Yoshimatsu, Yuta and Hamamura, Toshihiko and Anai, Kenta and Ueno, Midori and Yamaga, Satoru and Ueda, Issei and Terasawa, Takashi and Fujisaki, Akitaka and Chihara, Chihiro and Miyagi, Tomoyuki and Aoki, Takatoshi and Korogi, Yukunori},
	copyright = {Copyright {\copyright}2020 Elsevier B.V. All rights reserved.},
	crdt = {2020/07/30 06:00},
	date = {2020 Sep},
	date-added = {2022-12-08 17:26:56 -0500},
	date-modified = {2022-12-08 17:26:56 -0500},
	dcom = {20210317},
	dep = {20200723},
	doi = {10.1016/j.ejrad.2020.109188},
	edat = {2020/07/30 06:00},
	issn = {1872-7727 (Electronic); 0720-048X (Linking)},
	jid = {8106411},
	journal = {Eur J Radiol},
	jt = {European journal of radiology},
	keywords = {Diagnosis, Computer-Assisted; Femoral Fractures; Neural Networks, Computer; Radiography; Reference Standards},
	language = {eng},
	lid = {S0720-048X(20)30377-6 {$[$}pii{$]$}; 10.1016/j.ejrad.2020.109188 {$[$}doi{$]$}},
	lr = {20210317},
	mh = {Aged; Aged, 80 and over; Area Under Curve; *Deep Learning; Female; Hip Fractures/*diagnostic imaging; Humans; Image Processing, Computer-Assisted/methods; Magnetic Resonance Imaging/*methods; Male; Middle Aged; *Neural Networks, Computer; Pelvis/*diagnostic imaging; *ROC Curve; Radiographic Image Enhancement/*methods; Radiography, Abdominal/*methods; Tomography, X-Ray Computed/*methods},
	mhda = {2021/03/18 06:00},
	month = {Sep},
	oto = {NOTNLM},
	own = {NLM},
	pages = {109188},
	phst = {2020/04/26 00:00 {$[$}received{$]$}; 2020/07/12 00:00 {$[$}revised{$]$}; 2020/07/18 00:00 {$[$}accepted{$]$}; 2020/07/30 06:00 {$[$}pubmed{$]$}; 2021/03/18 06:00 {$[$}medline{$]$}; 2020/07/30 06:00 {$[$}entrez{$]$}},
	pii = {S0720-048X(20)30377-6},
	pl = {Ireland},
	pmid = {32721827},
	pst = {ppublish},
	pt = {Journal Article},
	sb = {IM},
	status = {MEDLINE},
	title = {The effect of deep convolutional neural networks on radiologists' performance in the detection of hip fractures on digital pelvic radiographs.},
	volume = {130},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1016/j.ejrad.2020.109188}}

@article{PRANATA201927,
	abstract = {Background and objectives
The calcaneus is the most fracture-prone tarsal bone and injuries to the surrounding tissue are some of the most difficult to treat. Currently there is a lack of consensus on treatment or interpretation of computed tomography (CT) images for calcaneus fractures. This study proposes a novel computer-assisted method for automated classification and detection of fracture locations in calcaneus CT images using a deep learning algorithm.
Methods
Two types of Convolutional Neural Network (CNN) architectures with different network depths, a Residual network (ResNet) and a Visual geometry group (VGG), were evaluated and compared for the classification performance of CT scans into fracture and non-fracture categories based on coronal, sagittal, and transverse views. The bone fracture detection algorithm incorporated fracture area matching using the speeded-up robust features (SURF) method, Canny edge detection, and contour tracing.
Results
Results showed that ResNet was comparable in accuracy (98%) to the VGG network for bone fracture classification but achieved better performance for involving a deeper neural network architecture. ResNet classification results were used as the input for detecting the location and type of bone fracture using SURF algorithm.
Conclusions
Results from real patient fracture data sets demonstrate the feasibility using deep CNN and SURF for computer-aided classification and detection of the location of calcaneus fractures in CT images.},
	author = {Yoga Dwi Pranata and Kuan-Chung Wang and Jia-Ching Wang and Irwansyah Idram and Jiing-Yih Lai and Jia-Wei Liu and I-Hui Hsieh},
	date-added = {2022-12-08 17:23:43 -0500},
	date-modified = {2022-12-08 17:23:43 -0500},
	doi = {https://doi.org/10.1016/j.cmpb.2019.02.006},
	issn = {0169-2607},
	journal = {Computer Methods and Programs in Biomedicine},
	keywords = {Convolutional neural networks, Calcaneus fracture, Computed tomography image, Residual network, Visual geometry group},
	pages = {27-37},
	title = {Deep learning and SURF for automated classification and detection of calcaneus fractures in CT images},
	url = {https://www.sciencedirect.com/science/article/pii/S0169260718314652},
	volume = {171},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0169260718314652},
	bdsk-url-2 = {https://doi.org/10.1016/j.cmpb.2019.02.006}}

@article{Thian:2019aa,
	abstract = {PURPOSE: To demonstrate the feasibility and performance of an object detection convolutional neural network (CNN) for fracture detection and localization on wrist radiographs. MATERIALS AND METHODS: Institutional review board approval was obtained with waiver of consent for this retrospective study. A total of 7356 wrist radiographic studies were extracted from a hospital picture archiving and communication system. Radiologists annotated all radius and ulna fractures with bounding boxes. The dataset was split into training (90%) and validation (10%) sets and used to train fracture localization models for frontal and lateral images. Inception-ResNet Faster R-CNN architecture was implemented as a deep learning model. The models were tested on an unseen test set of 524 consecutive emergency department wrist radiographic studies with two radiologists in consensus as the reference standard. Per-fracture, per-image (ie, per-view), and per-study sensitivity and specificity were determined. Area under the receiver operating characteristic curve (AUC) analysis was performed. RESULTS: The model detected and correctly localized 310 (91.2%) of 340 and 236 (96.3%) of 245 of all radius and ulna fractures on the frontal and lateral views, respectively. The per-image sensitivity, specificity, and AUC were 95.7% (95% confidence interval [CI]: 92.4%, 97.8%), 82.5% (95% CI: 77.4%, 86.8%), and 0.918 (95% CI: 0.894, 0.941), respectively, for the frontal view and 96.7% (95% CI: 93.6%, 98.6%), 86.4% (95% CI: 81.9%, 90.2%), and 0.933 (95% CI: 0.912, 0.954), respectively, for the lateral view. The per-study sensitivity, specificity, and AUC were 98.1% (95% CI: 95.6%, 99.4%), 72.9% (95% CI: 67.1%, 78.2%), and 0.895 (95% CI: 0.870, 0.920), respectively. CONCLUSION: The ability of an object detection CNN to detect and localize radius and ulna fractures on wrist radiographs with high sensitivity and specificity was demonstrated.{\copyright} RSNA, 2019.},
	address = {Department of Diagnostic Imaging (Y.L.T., P.J., D.S., V.E.Y.C.) and Department of Electrical and Computer Engineering (Y.L., R.T.T.), National University of Singapore, 5 Lower Kent Ridge Rd, Singapore 119074; and Science Division, Yale-NUS College, Singapore (R.T.T.).; Department of Diagnostic Imaging (Y.L.T., P.J., D.S., V.E.Y.C.) and Department of Electrical and Computer Engineering (Y.L., R.T.T.), National University of Singapore, 5 Lower Kent Ridge Rd, Singapore 119074; and Science Division, Yale-NUS College, Singapore (R.T.T.).; Department of Diagnostic Imaging (Y.L.T., P.J., D.S., V.E.Y.C.) and Department of Electrical and Computer Engineering (Y.L., R.T.T.), National University of Singapore, 5 Lower Kent Ridge Rd, Singapore 119074; and Science Division, Yale-NUS College, Singapore (R.T.T.).; Department of Diagnostic Imaging (Y.L.T., P.J., D.S., V.E.Y.C.) and Department of Electrical and Computer Engineering (Y.L., R.T.T.), National University of Singapore, 5 Lower Kent Ridge Rd, Singapore 119074; and Science Division, Yale-NUS College, Singapore (R.T.T.).; Department of Diagnostic Imaging (Y.L.T., P.J., D.S., V.E.Y.C.) and Department of Electrical and Computer Engineering (Y.L., R.T.T.), National University of Singapore, 5 Lower Kent Ridge Rd, Singapore 119074; and Science Division, Yale-NUS College, Singapore (R.T.T.).; Department of Diagnostic Imaging (Y.L.T., P.J., D.S., V.E.Y.C.) and Department of Electrical and Computer Engineering (Y.L., R.T.T.), National University of Singapore, 5 Lower Kent Ridge Rd, Singapore 119074; and Science Division, Yale-NUS College, Singapore (R.T.T.).},
	auid = {ORCID: 0000-0001-9899-205X; ORCID: 0000-0003-0427-8539},
	author = {Thian, Yee Liang and Li, Yiting and Jagmohan, Pooja and Sia, David and Chan, Vincent Ern Yao and Tan, Robby T},
	copyright = {2019 by the Radiological Society of North America, Inc.},
	crdt = {2021/05/03 06:28},
	date = {2019 Jan},
	date-added = {2022-12-08 17:09:39 -0500},
	date-modified = {2022-12-08 17:09:39 -0500},
	dep = {20190130},
	doi = {10.1148/ryai.2019180001},
	edat = {2019/01/30 00:00},
	issn = {2638-6100 (Electronic); 2638-6100 (Linking)},
	jid = {101746556},
	journal = {Radiol Artif Intell},
	jt = {Radiology. Artificial intelligence},
	language = {eng},
	lid = {10.1148/ryai.2019180001 {$[$}doi{$]$}; e180001},
	lr = {20220422},
	mhda = {2019/01/30 00:01},
	month = {Jan},
	number = {1},
	own = {NLM},
	pages = {e180001},
	phst = {2018/06/27 00:00 {$[$}received{$]$}; 2018/10/01 00:00 {$[$}revised{$]$}; 2018/12/10 00:00 {$[$}accepted{$]$}; 2021/05/03 06:28 {$[$}entrez{$]$}; 2019/01/30 00:00 {$[$}pubmed{$]$}; 2019/01/30 00:01 {$[$}medline{$]$}},
	pl = {United States},
	pmc = {PMC8017412},
	pmid = {33937780},
	pst = {epublish},
	pt = {Journal Article},
	status = {PubMed-not-MEDLINE},
	title = {Convolutional Neural Networks for Automated Fracture Detection and Localization on Wrist Radiographs.},
	volume = {1},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1148/ryai.2019180001}}

@article{Small:2021aa,
	abstract = {BACKGROUND AND PURPOSE: Multidetector CT has emerged as the standard of care imaging technique to evaluate cervical spine trauma. Our aim was to evaluate the performance of a convolutional neural network in the detection of cervical spine fractures on CT. MATERIALS AND METHODS: We evaluated C-spine, an FDA-approved convolutional neural network developed by Aidoc to detect cervical spine fractures on CT. A total of 665 examinations were included in our analysis. Ground truth was established by retrospective visualization of a fracture on CT by using all available CT, MR imaging, and convolutional neural network output information. The Ä¸ coefficients, sensitivity, specificity, and positive and negative predictive values were calculated with 95% CIs comparing diagnostic accuracy and agreement of the convolutional neural network and radiologist ratings, respectively, compared with ground truth. RESULTS: Convolutional neural network accuracy in cervical spine fracture detection was 92% (95% CI, 90%-94%), with 76% (95% CI, 68%-83%) sensitivity and 97% (95% CI, 95%-98%) specificity. The radiologist accuracy was 95% (95% CI, 94%-97%), with 93% (95% CI, 88%-97%) sensitivity and 96% (95% CI, 94%-98%) specificity. Fractures missed by the convolutional neural network and by radiologists were similar by level and location and included fractured anterior osteophytes, transverse processes, and spinous processes, as well as lower cervical spine fractures that are often obscured by CT beam attenuation. CONCLUSIONS: The convolutional neural network holds promise at both worklist prioritization and assisting radiologists in cervical spine fracture detection on CT. Understanding the strengths and weaknesses of the convolutional neural network is essential before its successful incorporation into clinical practice. Further refinements in sensitivity will improve convolutional neural network diagnostic utility.},
	address = {From the Departments of Neuroradiology (J.E.S., A.B.P., M.K.) Juan.E.Small@Lahey.org.; Radiology (P.O), Lahey Hospital and Medical Center, Burlington, Massachusetts.; From the Departments of Neuroradiology (J.E.S., A.B.P., M.K.).; From the Departments of Neuroradiology (J.E.S., A.B.P., M.K.).},
	auid = {ORCID: 0000-0002-4931-3564; ORCID: 0000-0003-4883-2925; ORCID: 0000-0002-5579-9855; ORCID: 0000-0002-7104-0101},
	author = {Small, J E and Osler, P and Paul, A B and Kunst, M},
	copyright = {{\copyright}2021 by American Journal of Neuroradiology.},
	crdt = {2021/07/13 17:23},
	date = {2021 Jul},
	date-added = {2022-12-08 16:39:44 -0500},
	date-modified = {2022-12-08 16:39:44 -0500},
	dcom = {20211012},
	dep = {20210401},
	doi = {10.3174/ajnr.A7094},
	edat = {2021/07/14 06:00},
	issn = {1936-959X (Electronic); 0195-6108 (Print); 0195-6108 (Linking)},
	jid = {8003708},
	journal = {AJNR Am J Neuroradiol},
	jt = {AJNR. American journal of neuroradiology},
	language = {eng},
	lid = {10.3174/ajnr.A7094 {$[$}doi{$]$}},
	lr = {20220716},
	mh = {Cervical Vertebrae/diagnostic imaging/injuries; Humans; Neural Networks, Computer; Retrospective Studies; Sensitivity and Specificity; *Spinal Fractures/diagnostic imaging; Tomography, X-Ray Computed},
	mhda = {2021/10/13 06:00},
	month = {Jul},
	number = {7},
	own = {NLM},
	pages = {1341--1347},
	phst = {2020/08/21 00:00 {$[$}received{$]$}; 2021/01/25 00:00 {$[$}accepted{$]$}; 2021/07/13 17:23 {$[$}entrez{$]$}; 2021/07/14 06:00 {$[$}pubmed{$]$}; 2021/10/13 06:00 {$[$}medline{$]$}},
	pii = {ajnr.A7094; 20-01239},
	pl = {United States},
	pmc = {PMC8324280},
	pmid = {34255730},
	pst = {ppublish},
	pt = {Journal Article},
	sb = {IM},
	status = {MEDLINE},
	title = {CT Cervical Spine Fracture Detection Using a Convolutional Neural Network.},
	volume = {42},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.3174/ajnr.A7094}}

@article{DBLP:journals/corr/abs-1812-00572,
	author = {Hyunkwang Lee and Myeongchan Kim and Synho Do},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-1812-00572.bib},
	date-added = {2022-12-06 17:09:13 -0500},
	date-modified = {2022-12-06 17:09:13 -0500},
	eprint = {1812.00572},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Tue, 01 Jan 2019 15:01:25 +0100},
	title = {Practical Window Setting Optimization for Medical Image Deep Learning},
	url = {http://arxiv.org/abs/1812.00572},
	volume = {abs/1812.00572},
	year = {2018},
	bdsk-url-1 = {http://arxiv.org/abs/1812.00572}}

@url{rsna-url,
	date-added = {2022-12-06 16:56:07 -0500},
	date-modified = {2022-12-06 17:03:07 -0500},
	keywords = {rsna},
	title = {Radiology Society of North America},
	url = {https://www.rsna.org/},
	bdsk-url-1 = {https://www.rsna.org/}}

@webpage{Fused-MBConv,
	author = {Suyog Gupta and Mingxing Tan},
	date-added = {2022-12-01 00:20:53 -0500},
	date-modified = {2022-12-01 00:22:28 -0500},
	keywords = {AutoML, Hardware, Image Classification, TPU},
	month = {Aug},
	title = {EfficientNet-EdgeTPU: Creating Accelerator-Optimized Neural Networks with AutoML},
	url = {https://ai.googleblog.com/2019/08/efficientnet-edgetpu-creating.html},
	year = {2019},
	bdsk-url-1 = {https://ai.googleblog.com/2019/08/efficientnet-edgetpu-creating.html},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAXY2l0YXRpb25zLzMzOTM3NzgwLm5iaWJPEQHsAAAAAAHsAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADfdBYFQkQAAf////8NMzM5Mzc3ODAubmJpYgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////9+3ypgAAAAAAAAAAAABAAMAAAogY3UAAAAAAAAAAAAAAAAACWNpdGF0aW9ucwAAAgB5LzpVc2VyczpzaWRkaGFydGhyYXlhYmhhcmFtOkRvY3VtZW50czptYXN0ZXJzOmNvbXA1OTc6Y2VydmljYWwtZnJhY3R1cmUtZGV0ZWN0aW9uOnJlcG9ydDp2ZXJzaW9uMjpjaXRhdGlvbnM6MzM5Mzc3ODAubmJpYgAADgAcAA0AMwAzADkAMwA3ADcAOAAwAC4AbgBiAGkAYgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAHdVc2Vycy9zaWRkaGFydGhyYXlhYmhhcmFtL0RvY3VtZW50cy9tYXN0ZXJzL2NvbXA1OTcvY2VydmljYWwtZnJhY3R1cmUtZGV0ZWN0aW9uL3JlcG9ydC92ZXJzaW9uMi9jaXRhdGlvbnMvMzM5Mzc3ODAubmJpYgAAEwABLwAAFQACABr//wAAAAgADQAaACQAPgAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAIu}}

@webpage{SEG:RandForest,
	author = {Lars Buitinck and Gilles Louppe},
	date-added = {2022-11-30 22:08:34 -0500},
	date-modified = {2022-12-01 00:24:24 -0500},
	title = {Scikit-learn: Machine Learning in Python, Random Forest Classifier},
	url = {https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html},
	bdsk-url-1 = {https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html}}

@inproceedings{DBLP:conf/icml/TanL21,
	author = {Mingxing Tan and Quoc V. Le},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/icml/TanL21.bib},
	booktitle = {Proceedings of the 38th International Conference on Machine Learning, {ICML} 2021, 18-24 July 2021, Virtual Event},
	date-added = {2022-11-30 22:02:09 -0500},
	date-modified = {2022-11-30 22:02:09 -0500},
	editor = {Marina Meila and Tong Zhang},
	pages = {10096--10106},
	publisher = {{PMLR}},
	series = {Proceedings of Machine Learning Research},
	timestamp = {Wed, 25 Aug 2021 17:11:17 +0200},
	title = {EfficientNetV2: Smaller Models and Faster Training},
	url = {http://proceedings.mlr.press/v139/tan21a.html},
	volume = {139},
	year = {2021},
	bdsk-url-1 = {http://proceedings.mlr.press/v139/tan21a.html}}

@article{Kim:2018aa,
	abstract = {AIM: To identify the extent to which transfer learning from deep convolutional neural networks (CNNs), pre-trained on non-medical images, can be used for automated fracture detection on plain radiographs. MATERIALS AND METHODS: The top layer of the Inception v3 network was re-trained using lateral wrist radiographs to produce a model for the classification of new studies as either "fracture" or "no fracture". The model was trained on a total of 11,112 images, after an eightfold data augmentation technique, from an initial set of 1,389 radiographs (695 "fracture" and 694 "no fracture"). The training data set was split 80:10:10 into training, validation, and test groups, respectively. An additional 100 wrist radiographs, comprising 50 "fracture" and 50 "no fracture" images, were used for final testing and statistical analysis. RESULTS: The area under the receiver operator characteristic curve (AUC) for this test was 0.954. Setting the diagnostic cut-off at a threshold designed to maximise both sensitivity and specificity resulted in values of 0.9 and 0.88, respectively. CONCLUSION: The AUC scores for this test were comparable to state-of-the-art providing proof of concept for transfer learning from CNNs in fracture detection on plain radiographs. This was achieved using only a moderate sample size. This technique is largely transferable, and therefore, has many potential applications in medical imaging, which may lead to significant improvements in workflow productivity and in clinical risk reduction.},
	address = {Medical Imaging Department, Royal Devon and Exeter Hospital, Barrack Road, Exeter EX2 5DW, UK. Electronic address: Daniel.kim@nhs.net.; Medical Imaging Department, Royal Devon and Exeter Hospital, Barrack Road, Exeter EX2 5DW, UK.},
	author = {Kim, D H and MacKinnon, T},
	copyright = {Copyright {\copyright}2017 The Royal College of Radiologists. Published by Elsevier Ltd. All rights reserved.},
	crdt = {2017/12/23 06:00},
	date = {2018 May},
	date-added = {2022-11-30 19:22:08 -0500},
	date-modified = {2022-11-30 19:22:08 -0500},
	dcom = {20190415},
	dep = {20171218},
	doi = {10.1016/j.crad.2017.11.015},
	edat = {2017/12/23 06:00},
	issn = {1365-229X (Electronic); 0009-9260 (Linking)},
	jid = {1306016},
	journal = {Clin Radiol},
	jt = {Clinical radiology},
	language = {eng},
	lid = {S0009-9260(17)30535-4 {$[$}pii{$]$}; 10.1016/j.crad.2017.11.015 {$[$}doi{$]$}},
	lr = {20191210},
	mh = {*Artificial Intelligence; Deep Learning; Diagnosis, Differential; Fractures, Bone/*diagnostic imaging; Humans; Machine Learning; Neural Networks, Computer; Radiographic Image Interpretation, Computer-Assisted; Sensitivity and Specificity},
	mhda = {2019/04/16 06:00},
	month = {May},
	number = {5},
	own = {NLM},
	pages = {439--445},
	phst = {2017/05/28 00:00 {$[$}received{$]$}; 2017/11/14 00:00 {$[$}accepted{$]$}; 2017/12/23 06:00 {$[$}pubmed{$]$}; 2019/04/16 06:00 {$[$}medline{$]$}; 2017/12/23 06:00 {$[$}entrez{$]$}},
	pii = {S0009-9260(17)30535-4},
	pl = {England},
	pmid = {29269036},
	pst = {ppublish},
	pt = {Journal Article},
	sb = {IM},
	status = {MEDLINE},
	title = {Artificial intelligence in fracture detection: transfer learning from deep convolutional neural networks.},
	volume = {73},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1016/j.crad.2017.11.015}}

@article{Chung:2018aa,
	abstract = {Background and purpose - We aimed to evaluate the ability of artificial intelligence (a deep learning algorithm) to detect and classify proximal humerus fractures using plain anteroposterior shoulder radiographs. Patients and methods - 1,891 images (1 image per person) of normal shoulders (n = 515) and 4 proximal humerus fracture types (greater tuberosity, 346; surgical neck, 514; 3-part, 269; 4-part, 247) classified by 3 specialists were evaluated. We trained a deep convolutional neural network (CNN) after augmentation of a training dataset. The ability of the CNN, as measured by top-1 accuracy, area under receiver operating characteristics curve (AUC), sensitivity/specificity, and Youden index, in comparison with humans (28 general physicians, 11 general orthopedists, and 19 orthopedists specialized in the shoulder) to detect and classify proximal humerus fractures was evaluated. Results - The CNN showed a high performance of 96% top-1 accuracy, 1.00 AUC, 0.99/0.97 sensitivity/specificity, and 0.97 Youden index for distinguishing normal shoulders from proximal humerus fractures. In addition, the CNN showed promising results with 65-86% top-1 accuracy, 0.90-0.98 AUC, 0.88/0.83-0.97/0.94 sensitivity/specificity, and 0.71-0.90 Youden index for classifying fracture type. When compared with the human groups, the CNN showed superior performance to that of general physicians and orthopedists, similar performance to orthopedists specialized in the shoulder, and the superior performance of the CNN was more marked in complex 3- and 4-part fractures. Interpretation - The use of artificial intelligence can accurately detect and classify proximal humerus fractures on plain shoulder AP radiographs. Further studies are necessary to determine the feasibility of applying artificial intelligence in the clinic and whether its use could improve care and outcomes compared with current orthopedic assessments.},
	address = {a Department of Orthopaedic Surgery and.; b Department of Dermatology , I-dermatology clinic, Seoul.; a Department of Orthopaedic Surgery and.; a Department of Orthopaedic Surgery and.; c Department of Radiology , Konkuk University School of Medicine , Seoul.; d Department of Orthopaedic Surgery , Kyungpook National University College of Medicine , Daegu , Korea.; e Department of Orthopaedic Surgery , Myungji Hospital , Goyang.; f Department of Orthopaedic Surgery , Kangwon National University College of Medicine , Chuncheon , Korea.; g Department of Othopaedic Surgery , National Police Hospital , Seoul.; h Department of Orthopaedic Surgery , Catholic University College of Medicine, Seoul, St Mary's Hospital , Seoul , Korea.; i Department of Orthopaedic Surgery , Dong-A University College of Medicine , Pusan.; j Center for Bionics, Korea Institute of Science and Technology , Seoul , Korea.},
	author = {Chung, Seok Won and Han, Seung Seog and Lee, Ji Whan and Oh, Kyung-Soo and Kim, Na Ra and Yoon, Jong Pil and Kim, Joon Yub and Moon, Sung Hoon and Kwon, Jieun and Lee, Hyo-Jin and Noh, Young-Min and Kim, Youngjun},
	crdt = {2018/03/27 06:00},
	date = {2018 Aug},
	date-added = {2022-11-30 19:22:08 -0500},
	date-modified = {2022-11-30 19:22:08 -0500},
	dcom = {20190325},
	dep = {20180326},
	doi = {10.1080/17453674.2018.1453714},
	edat = {2018/03/27 06:00},
	issn = {1745-3682 (Electronic); 1745-3674 (Print); 1745-3674 (Linking)},
	jid = {101231512},
	journal = {Acta Orthop},
	jt = {Acta orthopaedica},
	language = {eng},
	lid = {10.1080/17453674.2018.1453714 {$[$}doi{$]$}},
	lr = {20191210},
	mh = {Adult; Aged; Aged, 80 and over; Algorithms; Area Under Curve; Arthrography; *Deep Learning; Female; Humans; Male; Middle Aged; Shoulder Fractures/classification/*diagnostic imaging; Young Adult},
	mhda = {2019/03/26 06:00},
	month = {Aug},
	number = {4},
	own = {NLM},
	pages = {468--473},
	phst = {2018/03/27 06:00 {$[$}pubmed{$]$}; 2019/03/26 06:00 {$[$}medline{$]$}; 2018/03/27 06:00 {$[$}entrez{$]$}},
	pii = {1453714},
	pl = {Sweden},
	pmc = {PMC6066766},
	pmid = {29577791},
	pst = {ppublish},
	pt = {Evaluation Study; Journal Article; Multicenter Study; Video-Audio Media},
	sb = {IM},
	status = {MEDLINE},
	title = {Automated detection and classification of the proximal humerus fracture by using deep learning algorithm.},
	volume = {89},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1080/17453674.2018.1453714}}

@article{Olczak:2017aa,
	abstract = {Background and purpose - Recent advances in artificial intelligence (deep learning) have shown remarkable performance in classifying non-medical images, and the technology is believed to be the next technological revolution. So far it has never been applied in an orthopedic setting, and in this study we sought to determine the feasibility of using deep learning for skeletal radiographs. Methods - We extracted 256,000 wrist, hand, and ankle radiographs from Danderyd's Hospital and identified 4 classes: fracture, laterality, body part, and exam view. We then selected 5 openly available deep learning networks that were adapted for these images. The most accurate network was benchmarked against a gold standard for fractures. We furthermore compared the network's performance with 2 senior orthopedic surgeons who reviewed images at the same resolution as the network. Results - All networks exhibited an accuracy of at least 90% when identifying laterality, body part, and exam view. The final accuracy for fractures was estimated at 83% for the best performing network. The network performed similarly to senior orthopedic surgeons when presented with images at the same resolution as the network. The 2 reviewer Cohen's kappa under these conditions was 0.76. Interpretation - This study supports the use for orthopedic radiographs of artificial intelligence, which can perform at a human level. While current implementation lacks important features that surgeons require, e.g. risk of dislocation, classifications, measurements, and combining multiple exam views, these problems have technical solutions that are waiting to be implemented for orthopedics.},
	address = {a Department of Clinical Sciences , Karolinska Institutet , Danderyd Hospital.; b Radiology clinic, Danderyd Hospital, Danderyd Hospital AB.; c Department of Robotics, Perception and Learning (RPL), School of Computer Science and Communication , KTH Royal Institute of Technology , Stockholm , Sweden.; a Department of Clinical Sciences , Karolinska Institutet , Danderyd Hospital.; c Department of Robotics, Perception and Learning (RPL), School of Computer Science and Communication , KTH Royal Institute of Technology , Stockholm , Sweden.; b Radiology clinic, Danderyd Hospital, Danderyd Hospital AB.; a Department of Clinical Sciences , Karolinska Institutet , Danderyd Hospital.; a Department of Clinical Sciences , Karolinska Institutet , Danderyd Hospital.; a Department of Clinical Sciences , Karolinska Institutet , Danderyd Hospital.},
	author = {Olczak, Jakub and Fahlberg, Niklas and Maki, Atsuto and Razavian, Ali Sharif and Jilert, Anthony and Stark, Andr{\'e} and Sk{\"o}ldenberg, Olof and Gordon, Max},
	cin = {Acta Orthop. 2017 Dec;88(6):577. PMID: 29106336},
	crdt = {2017/07/07 06:00},
	date = {2017 Dec},
	date-added = {2022-11-30 19:20:38 -0500},
	date-modified = {2022-11-30 19:20:38 -0500},
	dcom = {20171109},
	dep = {20170706},
	doi = {10.1080/17453674.2017.1344459},
	edat = {2017/07/07 06:00},
	issn = {1745-3682 (Electronic); 1745-3674 (Print); 1745-3674 (Linking)},
	jid = {101231512},
	journal = {Acta Orthop},
	jt = {Acta orthopaedica},
	language = {eng},
	lid = {10.1080/17453674.2017.1344459 {$[$}doi{$]$}},
	lr = {20220408},
	mh = {*Artificial Intelligence; Fractures, Bone/*diagnosis; Humans; *Radiographic Image Enhancement; Radiography/*methods; Reproducibility of Results},
	mhda = {2017/11/10 06:00},
	month = {Dec},
	number = {6},
	own = {NLM},
	pages = {581--586},
	phst = {2017/07/07 06:00 {$[$}pubmed{$]$}; 2017/11/10 06:00 {$[$}medline{$]$}; 2017/07/07 06:00 {$[$}entrez{$]$}},
	pii = {iort-88-581},
	pl = {Sweden},
	pmc = {PMC5694800},
	pmid = {28681679},
	pst = {ppublish},
	pt = {Journal Article},
	sb = {IM},
	status = {MEDLINE},
	title = {Artificial intelligence for analyzing orthopedic trauma radiographs.},
	volume = {88},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1080/17453674.2017.1344459}}

@article{Minja:2018ud,
	abstract = {There is controversy regarding the optimal imaging strategy in adult blunt trauma patients for suspected cervical spine trauma. Some investigators recommend negative computed tomography (CT) alone to clear the cervical spine in adult blunt trauma patients, while others insist that MR imaging is necessary, especially among obtunded adult blunt trauma patients. CT is an excellent imaging modality for bony cervical spine injury; however, there is a nonzero rate of clinically significant cervical spine injuries missed on CT. MR imaging has high sensitivity for soft tissue cervical spine injuries, but low specificity for the rare isolated unstable ligamentous cervical spine injury.},
	address = {Department of Radiology and Biomedical Imaging, Yale University School of Medicine, 333 Cedar Street, New Haven, CT 06510, USA. Electronic address: frank.minja@yale.edu.; Department of Radiology and Biomedical Imaging, Yale University School of Medicine, 333 Cedar Street, New Haven, CT 06510, USA.; Department of Radiology and Biomedical Imaging, Yale University School of Medicine, 333 Cedar Street, New Haven, CT 06510, USA.},
	author = {Minja, Frank J and Mehta, Kushal Y and Mian, Ali Y},
	copyright = {Copyright {\copyright}2018 Elsevier Inc. All rights reserved.},
	crdt = {2018/07/16 06:00},
	date = {2018 Aug},
	date-added = {2022-11-30 18:48:12 -0500},
	date-modified = {2022-11-30 18:48:12 -0500},
	dcom = {20181211},
	dep = {20180608},
	doi = {10.1016/j.nic.2018.03.009},
	edat = {2018/07/17 06:00},
	issn = {1557-9867 (Electronic); 1052-5149 (Linking)},
	jid = {9211377},
	journal = {Neuroimaging Clin N Am},
	jt = {Neuroimaging clinics of North America},
	keywords = {Cervical; Ligamentous; Obtunded; Spine; Trauma; Unstable},
	language = {eng},
	lid = {S1052-5149(18)30034-0 {$[$}pii{$]$}; 10.1016/j.nic.2018.03.009 {$[$}doi{$]$}},
	lr = {20181211},
	mh = {Cervical Vertebrae/*injuries; Humans; *Magnetic Resonance Imaging; Spinal Injuries/*diagnostic imaging; *Tomography, X-Ray Computed},
	mhda = {2018/12/12 06:00},
	month = {Aug},
	number = {3},
	oto = {NOTNLM},
	own = {NLM},
	pages = {483--493},
	phst = {2018/07/16 06:00 {$[$}entrez{$]$}; 2018/07/17 06:00 {$[$}pubmed{$]$}; 2018/12/12 06:00 {$[$}medline{$]$}},
	pii = {S1052-5149(18)30034-0},
	pl = {United States},
	pmid = {30007757},
	pst = {ppublish},
	pt = {Journal Article; Review},
	sb = {IM},
	status = {MEDLINE},
	title = {Current Challenges in the Use of Computed Tomography and MR Imaging in Suspected Cervical Spine Trauma.},
	volume = {28},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1016/j.nic.2018.03.009}}

@article{Milby:2008tt,
	abstract = {OBJECT: Diagnosis of cervical spinal injury (CSI) is an essential aspect of the trauma evaluation. This task is especially difficult in patients who are not clinically able to be evaluated (unevaluable) because of distracting painful injuries, intoxication, or concomitant head injury. For this population, the appropriate use of advanced imaging techniques for cervical spinal clearance remains undetermined. This study was undertaken to estimate the prevalence of unstable CSI, particularly among patients in whom clinical evaluation is impossible or unreliable. METHODS: Estimates of the prevalence of CSI in populations consisting of all trauma patients, alert patients only, and clinically unevaluable patients only were determined by variance-weighted pooling of data from 65 publications (281,864 patients) that met criteria for review. RESULTS: The overall prevalence of CSI among all trauma patients was 3.7%. The prevalence of CSI in alert patients was 2.8%, whereas unevaluable patients were at increased risk of CSI with a prevalence of 7.7% (p = 0.007). Overall, 41.9% of all CSI cases were considered to exhibit instability. CONCLUSIONS: Trauma patients who are clinically unevaluable have a higher prevalence of CSI than alert patients. Knowledge of the prevalence and risk of such injuries may help establish an evidence-based approach to the detection and management of clinically occult CSI.},
	address = {Department of Neurosurgery, Hospital of the University of Pennsylvania, PA 19104, USA.},
	author = {Milby, Andrew H and Halpern, Casey H and Guo, Wensheng and Stein, Sherman C},
	crdt = {2008/11/05 09:00},
	date = {2008},
	date-added = {2022-11-30 18:41:21 -0500},
	date-modified = {2022-11-30 18:41:21 -0500},
	dcom = {20090408},
	doi = {10.3171/FOC.2008.25.11.E10},
	edat = {2008/11/05 09:00},
	issn = {1092-0684 (Electronic); 1092-0684 (Linking)},
	jid = {100896471},
	journal = {Neurosurg Focus},
	jt = {Neurosurgical focus},
	language = {eng},
	lid = {10.3171/FOC.2008.25.11.E10 {$[$}doi{$]$}},
	lr = {20220409},
	mh = {Databases, Factual/statistics \& numerical data; Humans; Prevalence; Spinal Cord Injuries/complications/*epidemiology; Wounds and Injuries/*epidemiology/etiology},
	mhda = {2009/04/09 09:00},
	number = {5},
	own = {NLM},
	pages = {E10},
	phst = {2008/11/05 09:00 {$[$}pubmed{$]$}; 2009/04/09 09:00 {$[$}medline{$]$}; 2008/11/05 09:00 {$[$}entrez{$]$}},
	pl = {United States},
	pmid = {18980470},
	pst = {ppublish},
	pt = {Journal Article; Review},
	rf = {98},
	sb = {IM},
	status = {MEDLINE},
	title = {Prevalence of cervical spinal injury in trauma.},
	volume = {25},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.3171/FOC.2008.25.11.E10}}

@article{bib1,
	author = {Campbell, S. L. and Gear, C. W.},
	journal = {Numer. {M}ath.},
	number = {2},
	pages = {173--196},
	title = {The index of general nonlinear {D}{A}{E}{S}},
	volume = {72},
	year = {1995}}

@article{bib2,
	author = {Slifka, M. K. and Whitton, J. L.},
	doi = {10.1007/s001090000086},
	journal = {J. {M}ol. {M}ed.},
	pages = {74--80},
	title = {Clinical implications of dysregulated cytokine production},
	volume = {78},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.1007/s001090000086}}

@article{bib3,
	author = {Hamburger, C.},
	journal = {Ann. Mat. Pura. Appl.},
	number = {2},
	pages = {321--354},
	title = {Quasimonotonicity, regularity and duality for nonlinear systems of partial differential equations},
	volume = {169},
	year = {1995}}

@book{bib4,
	address = {Boston},
	author = {Geddes, K. O. and Czapor, S. R. and Labahn, G.},
	publisher = {Kluwer},
	title = {Algorithms for {C}omputer {A}lgebra},
	year = {1992}}

@incollection{bib5,
	address = {New {Y}ork},
	author = {Broy, M.},
	booktitle = {Software Pioneers},
	editor = {Broy, M. and Denert, E.},
	pages = {10--13},
	publisher = {Springer},
	title = {Software engineering---from auxiliary to key technologies},
	year = {1992}}

@book{bib6,
	address = {New {Y}ork},
	editor = {Seymour, R. S.},
	publisher = {Plenum},
	title = {Conductive {P}olymers},
	year = {1981}}

@inproceedings{bib7,
	address = {Heidelberg},
	author = {Smith, S. E.},
	booktitle = {Neuromuscular junction. {H}andbook of experimental pharmacology},
	editor = {Zaimis, E.},
	pages = {593--660},
	publisher = {Springer},
	title = {Neuromuscular blocking drugs in man},
	volume = {42},
	year = {1976}}

@misc{bib8,
	author = {Chung, S. T. and Morris, R. L.},
	note = {Paper presented at the 3rd international symposium on the genetics of industrial microorganisms, University of {W}isconsin, {M}adison, 4--9 June 1978},
	title = {Isolation and characterization of plasmid deoxyribonucleic acid from Streptomyces fradiae},
	year = {1978}}

@misc{bib10,
	author = {Babichev, S. A. and Ries, J. and Lvovsky, A. I.},
	note = {Preprint at \url{https://arxiv.org/abs/quant-ph/0208066v1}},
	title = {Quantum scissors: teleportation of single-mode optical states by means of a nonlocal single photon},
	year = {2002}}

@article{bib11,
	archiveprefix = {arXiv},
	author = {Beneke, M. and Buchalla, G. and Dunietz, I.},
	eprint = {0707.3168},
	journal = {Phys. {L}ett.},
	pages = {132-142},
	primaryclass = {gr-gc},
	title = {Mixing induced {CP} asymmetries in inclusive {B} decays},
	volume = {B393},
	year = {1997}}

@softmisc{bib12,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {{https://ui.adsabs.harvard.edu/abs/2020ascl.soft06023S}},
	archiveprefix = {ascl},
	author = {Stahl, B.},
	eid = {ascl:2006.023},
	eprint = {2006.023},
	howpublished = {Astrophysics {S}ource {C}ode {L}ibrary},
	keywords = {Software},
	month = {Jun},
	pages = {ascl:2006.023},
	title = {deep{SIP}: deep learning of {S}upernova {I}a {P}arameters},
	version = {0.42},
	year = {2020}}
